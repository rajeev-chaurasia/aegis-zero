{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "5b8058e9",
            "metadata": {},
            "source": [
                "# Train Final Aegis Zero Model\n",
                "\n",
                "This notebook trains the **Production XGBoost Model** for the Aegis Zero AI Engine.\n",
                "\n",
                "**Dataset:** `dhoogla/cicids2017`\n",
                "\n",
                "**Instructions:**\n",
                "1. **Run All Cells**.\n",
                "2. The script will automatically download the dataset from Kaggle.\n",
                "   - *Fallback:* If auto-download fails (e.g., 403 Forbidden), please download the zip manually from Kaggle and upload it to the Files area.\n",
                "3. It will process the data and train the model (`ROC AUC > 0.99` expected).\n",
                "4. The final model `xgboost_final.joblib` will be downloaded automatically."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "ee74f867",
            "metadata": {},
            "outputs": [],
            "source": [
                "%pip install -q xgboost fastparquet pyarrow kaggle"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "16af5117",
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import glob\n",
                "import zipfile\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "import joblib\n",
                "from xgboost import XGBClassifier\n",
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn.metrics import classification_report, roc_auc_score\n",
                "\n",
                "# Configuration\n",
                "DATA_DIR = './data'\n",
                "MODEL_NAME = 'xgboost_final.joblib'\n",
                "os.makedirs(DATA_DIR, exist_ok=True)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "e6067988",
            "metadata": {},
            "outputs": [],
            "source": [
                "# 1. Data Acquisition (Auto-Magic)\n",
                "print(\"Checking for data...\")\n",
                "\n",
                "# Set up Kaggle Token\n",
                "os.makedirs(os.path.expanduser('~/.kaggle'), exist_ok=True)\n",
                "kaggle_token = '{\"username\":\"rajeevchaurasia\",\"key\":\"22ae91b46e46b6790b2c2d581ccebc02\"}'\n",
                "with open(os.path.expanduser('~/.kaggle/kaggle.json'), 'w') as f:\n",
                "    f.write(kaggle_token)\n",
                "os.chmod(os.path.expanduser('~/.kaggle/kaggle.json'), 0o600)\n",
                "\n",
                "try:\n",
                "    if not glob.glob(f'{DATA_DIR}/*.parquet') and not glob.glob(f'{DATA_DIR}/*.csv'):\n",
                "        print(\"Attempting Kaggle download (dhoogla/cicids2017)...\")\n",
                "        !kaggle datasets download -d dhoogla/cicids2017 --unzip -p {DATA_DIR}\n",
                "except Exception as e:\n",
                "    print(f\"Kaggle download warning: {e}\")\n",
                "\n",
                "# Manual Fallback: Check for Zips\n",
                "zip_files = glob.glob('*.zip')\n",
                "for zf in zip_files:\n",
                "    print(f\"Unzipping manual upload: {zf}...\")\n",
                "    with zipfile.ZipFile(zf, 'r') as zip_ref:\n",
                "        zip_ref.extractall(DATA_DIR)\n",
                "\n",
                "# Verify\n",
                "data_files = glob.glob(f'{DATA_DIR}/*.parquet') + glob.glob(f'{DATA_DIR}/*.csv')\n",
                "if not data_files:\n",
                "    # Deep search\n",
                "    data_files = glob.glob(f'{DATA_DIR}/**/*.parquet', recursive=True) + glob.glob(f'{DATA_DIR}/**/*.csv', recursive=True)\n",
                "\n",
                "if not data_files:\n",
                "    raise RuntimeError(\"CRITICAL: No data files found! Please upload the dataset zip file manualy.\")\n",
                "else:\n",
                "    print(f\"Found {len(data_files)} data files. Ready.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "7db67d1c",
            "metadata": {},
            "outputs": [],
            "source": [
                "# 2. Load & Explore\n",
                "dfs = []\n",
                "for f in data_files:\n",
                "    try:\n",
                "        if f.endswith('.parquet'):\n",
                "            df = pd.read_parquet(f)\n",
                "        else:\n",
                "            df = pd.read_csv(f)\n",
                "        dfs.append(df.sample(frac=0.3, random_state=42)) # Sample 30% for speed\n",
                "    except Exception as e:\n",
                "        print(f\"Skipping {f}: {e}\")\n",
                "\n",
                "full_df = pd.concat(dfs, ignore_index=True)\n",
                "print(f\"Loaded {len(full_df)} samples.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "28369c7e",
            "metadata": {},
            "outputs": [],
            "source": [
                "# 3. Preprocessing\n",
                "SELECTED_FEATURES = [\n",
                "    'Bwd Packet Length Std',\n",
                "    'Bwd Packet Length Mean',\n",
                "    'Average Packet Size',\n",
                "    'Flow Bytes/s',\n",
                "    'Flow Packets/s',\n",
                "    'Fwd IAT Mean',\n",
                "    'Fwd IAT Max',\n",
                "    'Fwd IAT Min',\n",
                "    'Fwd IAT Total',\n",
                "    'Total Fwd Packets',\n",
                "    'Subflow Fwd Packets',\n",
                "    'Avg Bwd Segment Size'\n",
                "]\n",
                "\n",
                "# Clean Column Names\n",
                "full_df.columns = full_df.columns.str.strip()\n",
                "col_map = {\n",
                "    'Avg Packet Size': 'Average Packet Size',\n",
                "    'Subflow Fwd Pkts': 'Subflow Fwd Packets',\n",
                "    'Tot Fwd Pkts': 'Total Fwd Packets'\n",
                "}\n",
                "full_df.rename(columns=col_map, inplace=True)\n",
                "\n",
                "# Clean Data\n",
                "full_df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
                "full_df.dropna(inplace=True)\n",
                "full_df['is_attack'] = full_df['Label'].apply(lambda x: 0 if str(x).lower() == 'benign' else 1)\n",
                "\n",
                "X = full_df[SELECTED_FEATURES]\n",
                "y = full_df['is_attack']\n",
                "print(f\"Features Shape: {X.shape}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "7fc12bf9",
            "metadata": {},
            "outputs": [],
            "source": [
                "# 4. Train Model\n",
                "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
                "\n",
                "n_pos = y_train.sum()\n",
                "n_neg = len(y_train) - n_pos\n",
                "scale_pos = n_neg / n_pos if n_pos > 0 else 1.0\n",
                "\n",
                "print(f\"Training XGBoost (Pos Weight: {scale_pos:.2f})...\")\n",
                "\n",
                "model = XGBClassifier(\n",
                "    n_estimators=200,\n",
                "    max_depth=6,\n",
                "    learning_rate=0.1,\n",
                "    subsample=0.8,\n",
                "    colsample_bytree=0.8,\n",
                "    scale_pos_weight=scale_pos,\n",
                "    n_jobs=-1,\n",
                "    random_state=42,\n",
                "    tree_method='hist' \n",
                ")\n",
                "\n",
                "model.fit(X_train, y_train)\n",
                "\n",
                "y_prob = model.predict_proba(X_test)[:, 1]\n",
                "print(f\"ROC AUC: {roc_auc_score(y_test, y_prob):.4f}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "abf1d8e0",
            "metadata": {},
            "outputs": [],
            "source": [
                "# 5. Export\n",
                "joblib.dump(model, MODEL_NAME)\n",
                "print(f\"Model saved: {MODEL_NAME}\")\n",
                "\n",
                "try:\n",
                "    from google.colab import files\n",
                "    files.download(MODEL_NAME)\n",
                "except:\n",
                "    print(\"Auto-download skipped (not in Colab).\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.10"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
